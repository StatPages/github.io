<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD><TITLE>Logistic Regression Calculating Page</TITLE>
<META content="text/html; charset=windows-1252" http-equiv=Content-Type>
<META content="(PICS-1.0 " http-equiv=PICS-Label 1)?? (SS~~000 r 
?1996.08.22T13:44+0000 on l ? safesurf www.classify.org http:>
<SCRIPT language=JavaScript>
<!-- hide this script tag's contents from old browsers

function Abs(x) { return Math.abs(x) }
function Sqrt(x) { return Math.sqrt(x) }
function Exp(x) { return Math.exp(x) }
function Ln(x) { return Math.log(x) }
function Power(x,n) { return Math.pow(x,n) }

var Pi = 3.141592653589793;
var PiD2 = Pi/2;

function ChiSq(x,n) {
    if(x>1000 | n>1000) { var q=Norm((Power(x/n,1/3)+2/(9*n)-1)/Sqrt(2/(9*n)))/2; if (x>n) {return q} else {return 1-q} }
    var p=Math.exp(-0.5*x); if((n%2)==1) { p=p*Math.sqrt(2*x/Pi) }
    var k=n; while(k>=2) { p=p*x/k; k=k-2 }
    var t=p; var a=n; while(t>1e-15*p) { a=a+2; t=t*x/a; p=p+t }
    return 1-p
    }

function Norm(z) { var q=z*z
    if(Abs(z)>7) {return (1-1/q+3/(q*q))*Exp(-q/2)/(Abs(z)*Sqrt(PiD2))} else {return ChiSq(q,1) }
    }

function ANorm(p) { var v=0.5; var dv=0.5; var z=0
	while(dv>1e-15) { z=1/v-1; dv=dv/2; if(Norm(z)>p) { v=v-dv } else { v=v+dv } }
	return z
	}

function Fmt(x) { var v;
	if(x>=0) { v="          "+(x+0.00005) } else { v="          "+(x-0.00005) }
	v = v.substring(0,v.indexOf(".")+5)
	return v.substring(v.length-10,v.length)
	}

function Fmt3(x) { var v;
	v = "   " + x;
	return v.substring(v.length-3,v.length)
	}

function Fmt9(x) { var v;
	v = "         " + x;
	return v.substring(v.length-9,v.length)
	}

function vFmt(x) { var v;
	if(x>=0) { v="              "+(x+0.0000005) } else { v="          "+(x-0.0000005) }
	v = v.substring(0,v.indexOf(".")+7)
	return v.substring(v.length-14,v.length)
	}

function Xlate(s,from,to) { var v = s;
	var l=v.indexOf(from);
	while(l>-1) {
		v = v.substring(0,l) + to + v.substring(l+1,v.length);
		l=v.indexOf(from)
		}
	return v
    }

function crArr(n) {
	this.length = n
	for (var i = 0; i < this.length; i++) { this[i] = 0 }
	}
	
function ix(j,k,nCols) { return j * nCols + k }

var CR = unescape("%0D");
var LF = unescape("%0A");
var Tb = unescape("%09");
var NL = CR + LF;

function Iterate(form) {

var i = 0; var j = 0; var k = 0; var l = 0;

var nC   = eval(form.cPts.value);
var nR   = eval(form.cVar.value);
var nP   = nR + 1;
var nP1  = nP + 1;
var sY0 = 0;
var sY1 = 0;
var sC = 0;
var cConfLev = form.cConfLev.value
var zc = ANorm( 1 - eval(cConfLev)/100 )

var X    = new crArr( nC * ( nR + 1 ) );
var Y0   = new crArr( nC );
var Y1   = new crArr( nC );
var xM   = new crArr( nR + 1 );
var xSD  = new crArr( nR + 1 );
var Par  = new crArr( nP );
var SEP  = new crArr( nP );
var Arr  = new crArr( nP * nP1 );

var da = Xlate(form.data.value,Tb,",");
form.data.value = da;
if( da.indexOf(NL)==-1 ) { if( da.indexOf(CR)>-1 ) { NL = CR } else { NL = LF } }

for (i = 0; i<nC; i++) {
	X[ix(i,0,nR+1)] = 1;
	l = da.indexOf(NL); if( l==-1 ) { l = da.length };
	var v = da.substring(0,l);
	da = da.substring(l+NL.length,da.length);
	for (j = 1; j<=nR; j++) {
		l = v.indexOf(","); if( l==-1 ) { l = v.length };
		x = eval(v.substring(0,l))
		X[ix(i,j,nR+1)] = x;
		v = v.substring(l+1,v.length);
		}
	if(form.Grouped.checked=="1")
		{
		l = v.indexOf(","); if( l==-1 ) { l = v.length };
		x = eval(v.substring(0,l))
		Y0[i] = x; sY0 = sY0 + x;
		v = v.substring(l+1,v.length);
		l = v.indexOf(","); if( l==-1 ) { l = v.length };
		x = eval(v.substring(0,l))
		Y1[i] = x; sY1 = sY1 + x;
		v = v.substring(l+1,v.length);
		}
		else
		{
		x = eval(v.substring(0,l));
		if ( x==0 ) { Y0[i] = 1; sY0 = sY0 + 1 } else { Y1[i] = 1; sY1 = sY1 + 1 }
		}
	sC = sC + (Y0[i] + Y1[i]);
	for (j = 1; j<=nR; j++) {
		x = X[ix(i,j,nR+1)];
		xM[j] = xM[j] + (Y0[i] + Y1[i])*x;
		xSD[j] = xSD[j] + (Y0[i] + Y1[i])*x*x;
		}
	}

var o = "Descriptives..." + NL;

o = o + ( NL + sY0 + " cases have Y=0; " + sY1 + " cases have Y=1." + NL );

o = o + ( NL + " Variable     Avg       SD    " + NL );
for (j = 1; j<=nR; j++) {
	xM[j]  = xM[j]  / sC;
	xSD[j] = xSD[j] / sC;
	xSD[j] = Sqrt( Abs( xSD[j] - xM[j] * xM[j] ) )
	o = o + (  "   " + Fmt3(j) + "    " + Fmt(xM[j]) + Fmt(xSD[j])+ NL );
	}
xM[0] = 0; xSD[0] = 1;

for (i = 0; i<nC; i++) {
	for (j = 1; j<=nR; j++) {
		X[ix(i,j,nR+1)] = ( X[ix(i,j,nR+1)] - xM[j] ) / xSD[j];
		}
	}

o = o + ( NL + "Iteration History..." );
form.output.value = o;

Par[0] = Ln( sY1 / sY0 );
for (j = 1; j<=nR; j++) {
	Par[j] = 0;
	}

var LnV = 0; var Ln1mV = 0;

var LLp = 2e+10;
var LL  = 1e+10;
var Fract = 0.1

while( Abs(LLp-LL)>0.0000001 ) {
	LLp = LL;
	LL = 0;
	for (j = 0; j<=nR; j++) {
		for (k = j; k<=nR+1; k++) {
			Arr[ix(j,k,nR+2)] = 0;
			}
		}
	
	for (i = 0; i<nC; i++) {	
		var v = Par[0];
		for (j = 1; j<=nR; j++) {
			v = v + Par[j] * X[ix(i,j,nR+1)];
			}
		if( v>15 ) { LnV = -Exp(-v); Ln1mV = -v; q = Exp(-v); v=Exp(LnV) }
			else { if( v<-15 ) {	LnV = v; Ln1mV = -Exp(v); q = Exp(v); v=Exp(LnV) }
				else { v = 1 / ( 1 + Exp(-v) ); LnV = Ln(v); Ln1mV = Ln(1-v); q = v*(1-v) }
			}
		LL = LL - 2*Y1[i]*LnV - 2*Y0[i]*Ln1mV;
		for (j = 0; j<=nR; j++) {
			var xij = X[ix(i,j,nR+1)];
			Arr[ix(j,nR+1,nR+2)] = Arr[ix(j,nR+1,nR+2)] + xij * ( Y1[i] * (1 - v) + Y0[i] * (-v) );
			for (k=j; k<=nR; k++) {
				Arr[ix(j,k,nR+2)] = Arr[ix(j,k,nR+2)] + xij * X[ix(i,k,nR+1)] * q * (Y0[i] + Y1[i]);
				}
			}
		}

	o = o + ( NL + "-2 Log Likelihood = " + Fmt( LL ) );
	if( LLp==1e+10 ) { LLn = LL; o = o + " (Null Model)" }
	form.output.value = o;

	for (j = 1; j<=nR; j++) {
		for (k=0; k<j; k++) {
			Arr[ix(j,k,nR+2)] = Arr[ix(k,j,nR+2)];
			}
		}

	for (i=0; i<=nR; i++) { var s = Arr[ix(i,i,nR+2)]; Arr[ix(i,i,nR+2)] = 1;
		for (k=0; k<=nR+1; k++) { Arr[ix(i,k,nR+2)] = Arr[ix(i,k,nR+2)] / s; }
		for (j=0; j<=nR; j++) {
			if (i!=j) { s = Arr[ix(j,i,nR+2)]; Arr[ix(j,i,nR+2)] = 0;
				for (k=0; k<=nR+1; k++) {
					Arr[ix(j,k,nR+2)] = Arr[ix(j,k,nR+2)] - s * Arr[ix(i,k,nR+2)];
					}
				}
			}
		}

	for( j=0; j<=nR; j++) {
		Par[j] = Par[j] + Fract * Arr[ix(j,nR+1,nR+2)];
		}
Fract = Fract + 0.1; if( Fract>1 ) { Fract = 1 }
	}

o = o + ( " (Converged)" + NL );
var CSq = LLn - LL;
o = o + ( NL + "Overall Model Fit..." + NL + "  Chi Square=" + Fmt(CSq) + ";  df=" + nR + ";  p=" + Fmt(ChiSq(CSq,nR)) + NL );

o = o + ( NL + "Coefficients, Standard Errors, Odds Ratios, and " + cConfLev + "% Confidence Limits..." + NL );
o = o + ( " Variable     Coeff.    StdErr       p          O.R.      Low  --  High" + NL );
for( j=1; j<=nR; j++) {
	Par[j] = Par[j] / xSD[j];
	SEP[j] = Sqrt( Arr[ix(j,j,nP+1)] ) / xSD[j];
	Par[0] = Par[0] - Par[j] * xM[j];
	o = o + ( "   " + Fmt3(j) + "    " + Fmt(Par[j]) + Fmt(SEP[j]) + Fmt( Norm(Abs(Par[j]/SEP[j])) ) );
	var ORc = Exp( Par[j] );
	var ORl = Exp( Par[j] - zc * SEP[j] );
	var ORh = Exp( Par[j] + zc * SEP[j] );
	o = o + ( "   " + Fmt(ORc) + Fmt(ORl) + Fmt(ORh) + NL );
	}
SEI = 0;
for (j = 0; j<=nR; j++) {
	if( j==0 ) { Xj = 1 } else { Xj = -xM[j]/xSD[j] }
	for (k=0; k<=nR; k++) {
		if( k==0 ) { Xk = 1 } else { Xk = -xM[k]/xSD[k] }
		SEI = SEI + Xj * Xk * Arr[ix(j,k,nR+2)];
		}
	}
SEI = Sqrt( SEI )
o = o + ( "Intercept " + Fmt(Par[0]) + Fmt(SEI) + Fmt( Norm(Abs(Par[0]/SEI)) ) );

o = o + ( NL + NL + "Predicted Probability of Outcome, with " + cConfLev + "% Confidence Limits..." )
if(form.Grouped.checked=="1")
	{ o = o + ( NL + "    X                n0           n1     Prob        Low  --  High" + NL ) }
	else
	{ o = o + ( NL + "    X                 Y     Prob        Low  --  High" + NL ) }
for (i = 0; i<nC; i++) {	
	v = Par[0];
	for (j = 1; j<=nR; j++) {
		x = xM[j] + xSD[j] * X[ix(i,j,nR+1)];
		v = v + Par[j] * x;
		o = o + Fmt(x);
		}
	p = 1 / ( 1 + Exp( -v ) );
	if(form.Grouped.checked=="1")
		{ o = o + ( "    " + Fmt9(Y0[i]) + "    " + Fmt9(Y1[i]) + Fmt(p) ) }
		else
		{ o = o + ( "    " + Fmt9(Y1[i]) + Fmt(p) ) }

SEY = 0;
for (j = 0; j<=nR; j++) {
	if( j==0 ) { Xj = 1 } else { Xj = X[ix(i,j,nR+1)] }
	for (k=0; k<=nR; k++) {
		if( k==0 ) { Xk = 1 } else { Xk = X[ix(i,k,nR+1)] }
		SEY = SEY + Xj * Xk * Arr[ix(j,k,nR+2)];
		}
	}
SEY = Sqrt( SEY )
o = o + ( " " + Fmt(1/(1+Exp(-(v-zc*SEY)))) + " " + Fmt(1/(1+Exp(-(v+zc*SEY)))) );
o = o + NL;
}
form.output.value = o;

}		

<!-- done hiding from old browsers -->
  </SCRIPT>

</HEAD>
<BODY bgColor=#ffffee>
<CENTER>
<H2><FONT color=blue>Logistic Regression</FONT> </H2>
<P>by John C. Pezzullo<BR>Revised 2015-07-22: Apply fractional shifts for the 
first few iterations, to increase robustness for ill-conditioned data.</P>
<P>This page performs logistic regression, in which a dichotomous outcome is 
predicted by one or more variables. The program generates the coefficients of a 
prediction formula (and standard errors of estimate and significance levels), 
and odds ratios (with confidence intervals). </P></CENTER>
<P>
<HR>

<FORM method=post>
<H3><FONT color=red><A name=Instructions>Instructions</A>:</FONT></H3>For 
detailed examples by Kevin M. Sullivan, <A href="logistix.html">click here</A> 
<OL>
  <LI>
  <P>Enter the <FONT color=blue>number of data points</FONT>: <INPUT size=4 
  value=10 name=cPts> (or, if summary data, the number of lines of data). </P>
  <LI>
  <P>Enter the <FONT color=blue>number of predictor variables</FONT>: <INPUT 
  size=2 value=1 name=cVar> </P>
  <LI>
  <P>Enter the <FONT color=blue>confidence level</FONT>: <INPUT size=6 value=95 
  name=cConfLev>% </P>
  <LI>
  <P>If you're entering <FONT color=blue>summary data</FONT>, check here <INPUT 
  type=checkbox value=1 name=Grouped> </P>
  <LI>
  <P>Type or paste data in the window below. <BR>Predictor variable(s) first, 
  then outcome variable (1 if event occurred; 0 if it did not occur). <BR>If 
  summary data box checked (Step 4), enter outcome as 2 columns: # of 
  non-occurrences, then # of occurrences. <BR>Columns must be separated by 
  commas or tabs. <BR>See <A href="logistix.html">Kevin Sullivan's page</A> for 
  more examples of how to enter data. </P></LI></OL>
<P><TEXTAREA rows=14 cols=80 name=data>1,0
2,0
3,0
4,0
5,1
6,0
7,1
8,0
9,1
10,1
</TEXTAREA><BR>
<OL start=5>
  <LI>
  <P><FONT color=blue>Click the <INPUT onclick=Iterate(this.form) type=button value=Solve> button;</FONT> 
  results will appear in the window below: </P></LI></OL>
<P><TEXTAREA rows=11 cols=80 name=output></TEXTAREA><BR>
<OL start=6>
  <LI>
  <P>To print out results, copy (Ctrl-C) and paste (Ctrl-V) the contents of the 
  results Window to a word processor or text editor, then print the results from 
  that program. For best appearance, use a fixed-width font like Courier. 
  </P></LI></OL>
<P>
<HR>

<H3><FONT color=blue>Questions or Problems?</FONT> </H3>
<H4>*** <B>Not getting correct results or blank results?</B> </H4>
<P>If you are not getting numeric results or an error message, please assure the 
following: 
<UL>
  <LI>For each record or line of data, the data must be separated by a 
  <B>comma</B> or <B>tab</B>; if there are just spaces between the data, you 
  will get an error message or output with no calculated values. 
  <LI>All data values must be numeric. Character data (such as "Y" or "Yes" or 
  "+") will not work. 
  <LI>The outcome variable must have a 1 or 0 coding. 
  <LI>There cannot be any blank lines in the data. 
  <LI>All records must have values for every predictor variable. </LI></UL>
<H3>*** One (or more) of my coefficients came out very large (and the standard 
error is even larger!). Why did this happen? </H3>
<P>This is probably due to what is called "the <I>perfect predictor</I> or the 
<I>complete separation</I> problem". This occurs when one of the predictor 
variables is perfectly divided into two distinct ranges for the two outcomes. 
For example, if you had an independent variable like Age, and everyone 
<B>above</B> age 50 <B>had</B> the outcome event, and everyone 50 and 
<B>below</B> did <B>not</B> have the event, then the logistic algorithm will not 
converge (the regression coefficient for Age will take off toward infinity). The 
same thing can happen with categorical predictors. And it gets even more 
insidious when there's more than one predictor. None of the variables by 
themselves may look like "perfect predictors", but some subset of them taken 
together might form a pattern in n-dimensional space that can be sliced into two 
regions where everyone in one region had outcome=1 and everyone in the other 
region had outcome=0. This isn't a flaw in the web page; it's just that the 
logistic model is simply not appropriate for the data. The true relationship is 
a "step function", not the smooth "S-shaped" function of the logistic model.) 
<P>*** <B>How do I copy and paste data?</B> 
<P><I><B>Copy data</B>:</I> In most programs, you identify the data you want to 
copy then go to Edit-&gt;Copy 
<P>&lt;<B><I>Paste data</I></B>: Open this logistic regression program; place 
the cursor in the data window and highlight the example data, then, in Windows, 
simultaneously press the <B>Ctrl</B> and <B>V </B>keys; Mac users press the 
<B>Command</B> and <B>V</B> keys. 
<P><B>*** Can I copy and paste from Excel?</B> 
<P>Yes, highlight the columns with the data, Edit-&gt;Copy the data, and paste 
into the Logistic data window. Note that when you paste data from Excel into the 
data window, the different columns of data will be separated by a tab. You 
cannot see the tab in the data window, but you can usually tell the difference 
between a tab and blank spaces by placing the cursor in a line of data, then 
move the cursor to the right one space of a time - a tab will make the cursor 
move many spaces. 
<HR>

<H3><FONT color=blue><A name=Background>Background Info</A> (just what is 
logistic regression, anyway?):</FONT> </H3>
<P><B>Ordinary</B> regression deals with finding a function that relates a 
<B>continuous</B> outcome variable (dependent variable <I>y</I>) to one or more 
predictors (independent variables <I>x</I><SUB>1</SUB>, <I>x</I><SUB>2</SUB>, 
etc.). Simple linear regression assumes a function of the form:<BR><I>y</I> = 
c<SUB>0</SUB> + c<SUB>1</SUB> * <I>x</I><SUB>1</SUB> + c<SUB>2</SUB> * 
<I>x</I><SUB>2</SUB> +...<BR>and finds the values of c<SUB>0</SUB>, 
c<SUB>1</SUB>, c<SUB>2</SUB>, etc. (c<SUB>0</SUB> is called the "intercept" or 
"constant term"). 
<P><B>Logistic</B> regression is a variation of ordinary regression, useful when 
the observed outcome is <B>restricted to two values</B>, which usually represent 
the occurrence or non-occurrence of some outcome event, (usually coded as 1 or 
0, respectively). It produces a formula that predicts the <B>probability of the 
occurrence</B> as a function of the independent variables. 
<P>Logistic regression fits a special s-shaped curve by taking the linear 
regression (above), which could produce any <I>y</I>-value between minus 
infinity and plus infinity, and transforming it with the function:<BR><I>p</I> = 
Exp(<I>y</I>) / ( 1 + Exp(<I>y</I>) )<BR>which produces <I>p</I>-values between 
0 (as <I>y</I> approaches minus infinity) and 1 (as <I>y</I> approaches plus 
infinity). This now becomes a special kind of <I>non-linear</I> regression, 
which is what this page performs. 
<P>Logistic regression also produces <I>Odds Ratios</I> (O.R.) associated with 
each predictor value. The <I>odds</I> of an event is defined as the probability 
of the outcome event <B>occurring</B> divided by the probability of the event 
<B>not occurring</B>. The odds ratio for a predictor tells the relative amount 
by which the odds of the outcome increase (O.R. greater than 1.0) or decrease 
(O.R. less than 1.0) when the value of the predictor value is increased by 1.0 
units. 
<P>
<HR>

<H3>
<P align=left><FONT size=+0><B><A name=Techie>Techie-stuff</A> (for those who 
might be interested): </B></FONT></H3>
<P>This page contains a straightforward <I>JavaScript</I> implementation of a 
standard iterative method to maximize the Log Likelihood Function (LLF), defined 
as the sum of the logarithms of the predicted probabilities of occurrence for 
those cases where the event occurred and the logarithms of the predicted 
probabilities of non-occurrence for those cases where the event did not occur. 
<P>Maximization is by Newton's method, with a very simple elimination algorithm 
to invert and solve the simultaneous equations. Central-limit estimates of 
parameter standard errors are obtained from the diagonal terms of the inverse 
matrix. Odds Ratios and their confidence limits are obtained by exponentiating 
the parameters and their lower and upper confidence limits, approximated by +/- 
1.96 standard errors (for 95% limits). 
<P>No special convergence-acceleration techniques are used. For improved 
precision, the independent variables are temporarily converted to "standard 
scores" ( value - Mean ) / StdDev. The <I>Null Model</I> is used as the starting 
guess for the iterations -- all parameter coefficients are zero, and the 
intercept is the logarithm of the ratio of the number of cases with <I>y</I>=1 
to the number with <I>y</I>=0. The quantity -2*Ln(Likelihood) is displayed for 
the null model, for each step of the iteration, and for the final (converged 
model). Convergence is not guaranteed, but this page should work properly with 
most practical problems that arise in real-world situations. 
<P>This implementation has no predefined limits for the number of independent 
variables or cases. The actual limits are probably dependent on your web 
browser's available memory and other browser-specific restrictions. 
<P>Reference: <I>Applied Logistic Regression</I>, by D.W. Hosmer and S. 
Lemeshow. 1989, John Wiley &amp; Sons, New York 
<P align=center>
<HR>
<BR>
<CENTER>Return to the <A href="index.html">Interactive Statistics</A> page or to 
the <A href="JCPhome.html">JCP Home Page</A></CENTER><BR></FORM></BODY></HTML>
